---
title: "Quality of Exercise Based on Movements from Activity Monitors:"
author: "Courtney H."
date: "Saturday, February 21, 2015"
output: html_document
---

```{r echo = FALSE, results='hide'}
library(caret)
library(randomForest)
set.seed(123)
train  <- read.csv("pml-training.csv", header=TRUE, na.strings = c("", " ","NA"))
test <- read.csv("pml-testing.csv",header=TRUE,na.strings = c("", " ","NA"))

train2 <- train[, !apply(train, 2, function(x) any(is.na(x)))]
train3 <- train2[,-c(1:7)]
inTrain <- createDataPartition(y=train3$classe, p = .7, list = FALSE)

trainTrain <- train3[inTrain,]
trainTest <- train3[-inTrain,]

model <- randomForest(classe~.,data = trainTrain, ntree=500)

pred <- predict(model,trainTest)
trainTest$predRight <- pred == trainTest$classe

predFinal <- predict(model,test)
```


###Goal:
Determine the quality of lifting dumbbells by predicting the type of exercise task completed (Class A - E, where A is the correct method and the others are common mistakes)  using the predictor variables provided in the exercise data set. (**ref:** http://groupware.les.inf.puc-rio.br/har)

###Model Building:
Decided to use the RandomForest model provided from the Coursera course because of its high accuracy and credibility as one of the top performing algorithms. In order to accurately build the model first I completed the following steps:

1. Cleaning the “training” data set: removed columns with blank entries and NA’s

2. Eliminated the first 7 columns of the data set because they were descriptive data or actual user information not needed in predictions

3. Separated “training” set into a 70/30 split in order to “cross-validate”

###Cross-Validation ,Out of Sample Error & Reasoning:
Running the “randomForest” algorithm, the output below shows our expected error rate is about .58%. 

``` {r echo=FALSE}
model
```


Testing the model on the training test set, we can see that our model had ~99% accuracy.

```{r echo = FALSE}
table(pred,trainTest$classe)
```


The model seems to be over-fitting because of the high accuracy, however, when applying the model to the given “test” set all predictors were correct. I understand that this isn’t the best way to determine a model because over-fitting is bad, but if we had more datasets to test on I would be able to test the model more. 